# GSERM St.Gallen 2019 - Applied Deep Learning in Python

This repository is a partial fulfillment ('Practical Work') for the completion of GSERM St.Gallen 2019 "Applied Deep Learning in Python" course.
The reflection paper can be found [here](https://github.com/iomz/gserm19-adl/releases/download/v1/GSERM_19_Applied_Deep_Learning_in_Python.pdf).

## Overview

`Explain your data, problem, and what you want to predict (also unsupervised problem is possible)`

This project implemented an object detection deep learning model to provide a classifier of artefacts in a video streaming from [ZED stereo camera](https://www.stereolabs.com/zed/).
Previously, I have been working on a Fault Detection \& Diagnosis Mixed Reality application for industrial settings such as manufacture assembly automation with robot arms.
This application assumed that the spatial coordinates of artefacts that the robot interacts with can be tracked by a means of Computer Vision, but was never implemented and has been deployed with synthetic data generated by the monitoring system.

![UI for the FDD application](https://i.imgur.com/dxt51w4.png)

I was inspired by [YOLOv3](https://github.com/AlexeyAB/darknet) introduced during the course and decided to implement the missing component by combining YOLO with the ZED stereo camera to translate the inferred object bounding box's coordinates for my Mixed Reality application to achieve a more realistically aligned methodology.

`Define approach: Why did you choose this model? Does the model fit your problem?`

Besides YOLO, there are other alternatives to implement such classifiers: Google's [Cloud AutoML](https://cloud.google.com/automl/), [SSD in TensorFlow](https://github.com/balancap/SSD-Tensorflow), etc. I chose YOLO simply because ZED's SDK is compatible with YOLOv3 and I found a useful sample script from Stereolabs to start from.

## 0. Setup

To fully reproduce the results from this project, you need a local environment. This sections explains how to setup a development environment on [NVIDIA Jetson AGX Xavier Developer Kit](https://developer.nvidia.com/embedded/jetson-agx-xavier-developer-kit).

### Install JetPack 4.2 on AGX Xavier
The official documentation is [here](https://docs.nvidia.com/sdk-manager/install-with-sdkm-jetson/index.html).

To flush JetPack OS Image, you need a linux host machine. In my case, I didn't have access to a physical machine with Linux at the time being so I created a VM with [Oracle VM VirtualBox](https://www.virtualbox.org/) on my macOS. The below is the configuration of the host machne -- note that you'll need approx 30 GB of storage to complete the whole SDK instllation process. Once you spin up the VM, follow the instructions provided by NVIDIA in the above documentation.

![VirtualBox](https://i.imgur.com/pURffqb.png)

### Install ZED SDK v2.8.3
The official documentation is [here](https://www.stereolabs.com/docs/getting-started/installation/).

Fortunately, ZED SDK v2.8.3 is officially supported with JetPack 4.2, so download the installation script from [here](https://www.stereolabs.com/developers/release/#sdkdownloads_anchor<Paste>) and simply run it. To verify the installation, try one of the tools came with the SDK. The below is a screenshot from 'ZED Depth Viewer' which can be found in `/usr/local/zed/tools`.

![Depth Viewer](https://i.imgur.com/qBPtVvm.png)

### Build Darknet YOLOv3

## 1. Get your data

https://github.com/iomz/gserm19-adl/releases/download/v1/artefact-recording.mp4

## 2. Prepare your data

![Yolo Mark](https://i.imgur.com/eFrxA4W.png)

https://github.com/iomz/gserm19-adl/releases/download/v1/artefact-training-data.tgz

## 3. Define and build the model

![Netron Visualization of the resulting model](https://i.imgur.com/QwgJLkk.png)

## 4. Train

![Average loss over iterations](https://i.imgur.com/cCqRsNm.png)

https://github.com/iomz/gserm19-adl/releases/download/v1/yolov3-artefact_final.weights
https://github.com/iomz/gserm19-adl/releases/download/v1/frozen_yolov3-artefact.pb

## 5. Evaluate

`Evaluate the outcome: Make sure to cover overfitting. Did it learn the problem? Did it generalize?`

## 6. Modify + Repeat

`How can you improve it? Log all the experiments (it would be optimal to create new files for bigger changes like using a completely different model) and using comments and editing changes in the same file for smaller changes.`

`Why did you chose to modify what? Quality of answer is important.`

## Author

Iori Mizutani - University of St. Gallen 
