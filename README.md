# GSERM St.Gallen 2019 - Applied Deep Learning in Python

This repository is a partial fulfillment ('Practical Work') for the completion of GSERM St.Gallen 2019 "Applied Deep Learning in Python" course.
The reflection paper can be found [here](https://github.com/iomz/gserm19-adl/releases/download/v1/GSERM_19_Applied_Deep_Learning_in_Python.pdf).

# Object Detection and Localization with Darknet and Stereo Camera

## Overview

`Explain your data, problem, and what you want to predict (also unsupervised problem is possible)`

This project implemented an object detection deep learning model to provide a classifier of artefacts in a video streaming from [ZED stereo camera](https://www.stereolabs.com/zed/).
Previously, I have been working on a Fault Detection \& Diagnosis Mixed Reality application for industrial settings such as manufacture assembly automation with robot arms.
This application assumed that the spatial coordinates of "artefacts" (in this project, I used [sensor tags](http://www.ti.com/tool/TIDC-CC2650STK-SENSORTAG#) from Texas Instrument for the dataset) that the robot interacts with can be tracked by a means of Computer Vision, but was never implemented and has been deployed with synthetic data generated by the monitoring system.

![UI for the FDD application](https://i.imgur.com/dxt51w4.png)

I was inspired by [YOLOv3](https://github.com/AlexeyAB/darknet) introduced during the course and decided to implement the missing component by combining darknet with the ZED stereo camera to translate the inferred object bounding box's coordinates for my Mixed Reality application to achieve a more realistically aligned methodology.

`Define approach: Why did you choose this model? Does the model fit your problem?`

Besides Darknet, there are other alternatives to implement such classifiers: Google's [Cloud AutoML](https://cloud.google.com/automl/), [SSD in TensorFlow](https://github.com/balancap/SSD-Tensorflow), etc. I chose Darknet/YOLO simply because ZED's SDK is compatible with YOLOv3 and I found a useful [sample script](https://github.com/stereolabs/zed-yolo/tree/master/zed_python_sample) from Stereolabs to start from.

## 0. Setup

To fully reproduce the results from this project, you need a local environment. This section explains how to setup a development environment on [NVIDIA Jetson AGX Xavier Developer Kit](https://developer.nvidia.com/embedded/jetson-agx-xavier-developer-kit).

### Install JetPack 4.2 on AGX Xavier
The official documentation is [here](https://docs.nvidia.com/sdk-manager/install-with-sdkm-jetson/index.html).

To flush JetPack OS Image, you need a linux host machine. In my case, I didn't have access to a physical machine with Linux at the time being so I created a VM with [Oracle VM VirtualBox](https://www.virtualbox.org/) on my macOS. The below is the configuration of the host machne -- note that you'll need approx 30 GB of storage to complete the whole SDK instllation process. Once you spin up the VM, follow the instructions provided by NVIDIA in the above documentation.

![VirtualBox](https://i.imgur.com/pURffqb.png)

### Install ZED SDK v2.8.3
The official documentation is [here](https://www.stereolabs.com/docs/getting-started/installation/).

Fortunately, JetPack 4.2 is officially supported with ZED SDK v2.8.3, so simply download and run the installation script from [here](https://www.stereolabs.com/developers/release/#sdkdownloads_anchor). To verify the installation, try one of the tools came with the SDK. The below is a screenshot from 'ZED Depth Viewer' which can be found in `/usr/local/zed/tools`.

![Depth Viewer](https://i.imgur.com/qBPtVvm.png)

### Build Darknet YOLOv3
The original repository is https://github.com/AlexeyAB/darknet#requirements . Install the required packages and compile from the source.

For this project, I cloned the repository to `$HOME` and the rest assumes that you have `darknet` directory there.

```sh
% cd ~ && git clone git@github.com:AlexeyAB/darknet.git %% cd darknet
# Replace the `Makefile` with `darknet/Makefile` in this repository
% make
```

## 1. Get your data

To generate a training set of artefact images, I filmed a video with a webcam and recorded in mp4 format.

The original video used for training my final model can be found at: https://github.com/iomz/gserm19-adl/releases/download/v1/artefact-recording.mp4

## 2. Prepare your data

I used [Yolo_mark](https://github.com/AlexeyAB/Yolo_mark) to divide the video into frames and label each image.

After installing `Yolo_mark`, extract frames from the video file.

```sh
% ./yolo_mark ~/artefact-data/img cap_video ~/artefact-recording.mp4 4
```

Then, manually label each image.

```sh
% ./yolo_mark ~/artefact-data/img ~/artefact-data/train.txt ~/artefact-data/obj.names
```

The below is a screenshot from Yolo_mark running on Xavier labeling the dataset.
![Yolo Mark](https://i.imgur.com/eFrxA4W.png)

The labeled dataset used for training my final model contains 294 images, which can be found at: https://github.com/iomz/gserm19-adl/releases/download/v1/artefact-training-data.tgz

## 3. Define and build the model

To train the model of a single object classifier with fast inferencing, I trained a 24-layer model based on pre-trained weights for the convolutional layers from [darknet53](https://pjreddie.com/darknet/imagenet/#darknet53) (http://pjreddie.com/media/files/darknet53.conv.74).

The configuration of the model is defined in `darknet/yolov3-artefact.cfg` in this repository. [This thread](https://stackoverflow.com/questions/50390836/understanding-darknets-yolo-cfg-config-files) from stackoverflow was helpful to understand each parameter.

The below is the visualization of the resulting model using [Netron](https://github.com/lutzroeder/netron). In order to generate this graph, I converted the weights file into `.pb` format by `import_pb_to_tensorboard.py` from TensorFlow (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/import_pb_to_tensorboard.py). The actual `.pb` file can be found at https://github.com/iomz/gserm19-adl/releases/download/v1/frozen_yolov3-artefact.pb.

![Netron Visualization of the resulting model](https://i.imgur.com/QwgJLkk.png)

## 4. Train

As defined in the model configuration file from the previous step, I have trained the model for 35020 iterations with the learning rate adjustment after 15000 and 29000 batches. The below is the plot of the average loss throughout the iterations.

![Average loss over iterations](https://i.imgur.com/cCqRsNm.png)

The final trained model is `darknet/yolov3-artefact_final.weights` in this repository.

## 5. Evaluate

`Evaluate the outcome: Make sure to cover overfitting. Did it learn the problem? Did it generalize?`

I tested the trained model to detect the artefact in the video streaming both from a webcam and from ZED stereo camera.

To test with a live-feed from a webcam (running at http://localhost:8081)

```sh
% ./darknet detector demo artefact-data/obj.data yolov3-artefact.cfg backup/yolov3-artefact_final.weights http://localhost:8081 -i 0 -thresh 0.3
```

![webcam](https://i.imgur.com/Q0Nz8G5.png)

To test with ZED, run the script `darknet_zed.sh` in this repository. The following snippet is from a successful running:

![ZED](https://i.imgur.com/fMgIiFf.png)

```sh
xavier:~/darknet % LD_LIBRARY_PATH=./:/usr/local/cuda/lib64 ./uselib artefact-data/obj.names yolov3-artefact.cfg backup/yolov3-artefact_final.weights zed_camera

 Used GPU 0
   layer   filters  size/strd(dil)      input                output
   0 conv     16       3 x 3/ 1    416 x 416 x   3 ->  416 x 416 x  16 0.150 BF
   1 max               2 x 2/ 2    416 x 416 x  16 ->  208 x 208 x  16 0.003 BF
   2 conv     32       3 x 3/ 1    208 x 208 x  16 ->  208 x 208 x  32 0.399 BF
   3 max               2 x 2/ 2    208 x 208 x  32 ->  104 x 104 x  32 0.001 BF
   4 conv     64       3 x 3/ 1    104 x 104 x  32 ->  104 x 104 x  64 0.399 BF
   5 max               2 x 2/ 2    104 x 104 x  64 ->   52 x  52 x  64 0.001 BF
   6 conv    128       3 x 3/ 1     52 x  52 x  64 ->   52 x  52 x 128 0.399 BF
   7 max               2 x 2/ 2     52 x  52 x 128 ->   26 x  26 x 128 0.000 BF
   8 conv    256       3 x 3/ 1     26 x  26 x 128 ->   26 x  26 x 256 0.399 BF
   9 max               2 x 2/ 2     26 x  26 x 256 ->   13 x  13 x 256 0.000 BF
  10 conv    512       3 x 3/ 1     13 x  13 x 256 ->   13 x  13 x 512 0.399 BF
  11 max               2 x 2/ 1     13 x  13 x 512 ->   13 x  13 x 512 0.000 BF
  12 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF
  13 conv    256       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 256 0.089 BF
  14 conv    512       3 x 3/ 1     13 x  13 x 256 ->   13 x  13 x 512 0.399 BF
  15 conv     18       1 x 1/ 1     13 x  13 x 512 ->   13 x  13 x  18 0.003 BF
  16 yolo
[yolo] params: iou loss: mse, iou_norm: 0.75, cls_norm: 1.00, scale_x_y: 1.00
  17 route  13
  18 conv    128       1 x 1/ 1     13 x  13 x 256 ->   13 x  13 x 128 0.011 BF
  19 upsample                 2x    13 x  13 x 128 ->   26 x  26 x 128
  20 route  19 8
  21 conv    256       3 x 3/ 1     26 x  26 x 384 ->   26 x  26 x 256 1.196 BF
  22 conv     18       1 x 1/ 1     26 x  26 x 256 ->   26 x  26 x  18 0.006 BF
  23 yolo
[yolo] params: iou loss: mse, iou_norm: 0.75, cls_norm: 1.00, scale_x_y: 1.00
Total BFLOPS 5.448
 Allocate additional workspace_size = 52.43 MB
Loading weights from backup/yolov3-artefact_final.weights...
 seen 64
Done!

 try to allocate additional workspace_size = 52.43 MB
 CUDA allocate done!
object names loaded
input image or video filename: ZED 3D Camera SUCCESS

 Video size: [1280 x 720]
 t_write exit
 t_network exit
```

Finally, run the python script for extracting artefacts' coordinates. This script is a modified version of [this](https://github.com/stereolabs/zed-yolo/tree/master/zed_python_sample).

```sh
% python3 darknet_zed.py
```

## 6. Modify + Repeat

`How can you improve it? Log all the experiments (it would be optimal to create new files for bigger changes like using a completely different model) and using comments and editing changes in the same file for smaller changes.`

`Why did you choose to modify what? Quality of answer is important.`

Throughout the data preparation and the training process, there are mainly three improvements that I have made.

### Dataset

In the first few trials, the model only detected the artefact when holding it with a hand. This was an overfitting problem due to the biased dataset (almost all the training image included my hand holding the artefact). I then added other images withou my hand to the dataset and solved the problem.

The overfitting model is here: 

### Model configuration

Initially I started from `yolov3-tiny.cfg` in 

### Iteration

The darknet documentation suggests that 

## Author

Iori Mizutani - University of St. Gallen
